<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="数据清洗,数据预处理," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="本文介绍进行数据挖掘和机器学习之前需要对数据进行的基本预处理方法[1]，包括量纲标准化、有偏数据的检测和转换、离群点的检测和处理、降维、特征选择、缺失值处理、去除或者增加predictor、样本类别失衡处理，针对不同的数据挖掘任务和机器学习模型需要采用相应的预处理方法。采用UCI上的adult数据集作为例子分析。">
<meta property="og:type" content="article">
<meta property="og:title" content="数据预处理基本方法">
<meta property="og:url" content="http://yoursite.com/2017/04/06/数据预处理基本方法/index.html">
<meta property="og:site_name" content="二颗苹果">
<meta property="og:description" content="本文介绍进行数据挖掘和机器学习之前需要对数据进行的基本预处理方法[1]，包括量纲标准化、有偏数据的检测和转换、离群点的检测和处理、降维、特征选择、缺失值处理、去除或者增加predictor、样本类别失衡处理，针对不同的数据挖掘任务和机器学习模型需要采用相应的预处理方法。采用UCI上的adult数据集作为例子分析。">
<meta property="og:image" content="http://onvolufm1.bkt.clouddn.com/smsspam_iForest.png">
<meta property="og:image" content="http://onvolufm1.bkt.clouddn.com/PCA.png">
<meta property="og:image" content="http://onvolufm1.bkt.clouddn.com/PCA_scree_plot.png">
<meta property="og:image" content="http://onvolufm1.bkt.clouddn.com/PCA_rebuildgrey.png">
<meta property="og:image" content="http://onvolufm1.bkt.clouddn.com/PCA_ratio.png">
<meta property="og:image" content="http://onvolufm1.bkt.clouddn.com/histogram_capital.png">
<meta property="og:updated_time" content="2017-05-20T12:07:04.962Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="数据预处理基本方法">
<meta name="twitter:description" content="本文介绍进行数据挖掘和机器学习之前需要对数据进行的基本预处理方法[1]，包括量纲标准化、有偏数据的检测和转换、离群点的检测和处理、降维、特征选择、缺失值处理、去除或者增加predictor、样本类别失衡处理，针对不同的数据挖掘任务和机器学习模型需要采用相应的预处理方法。采用UCI上的adult数据集作为例子分析。">
<meta name="twitter:image" content="http://onvolufm1.bkt.clouddn.com/smsspam_iForest.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/04/06/数据预处理基本方法/"/>





  <title> 数据预处理基本方法 | 二颗苹果 </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">二颗苹果</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/06/数据预处理基本方法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="周君君">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://onvolufm1.bkt.clouddn.com/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="二颗苹果">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                数据预处理基本方法
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-06T11:00:39+08:00">
                2017-04-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/目录/数据挖掘/" itemprop="url" rel="index">
                    <span itemprop="name">数据挖掘</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          
              <div class="post-description">
                  本文介绍进行数据挖掘和机器学习之前需要对数据进行的基本预处理方法[1]，包括量纲标准化、有偏数据的检测和转换、离群点的检测和处理、降维、特征选择、缺失值处理、去除或者增加predictor、样本类别失衡处理，针对不同的数据挖掘任务和机器学习模型需要采用相应的预处理方法。采用UCI上的adult数据集作为例子分析。
              </div>
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="量纲标准化"><a href="#量纲标准化" class="headerlink" title="量纲标准化"></a>量纲标准化</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>如果一个数据集中包括了工资和年龄这两个预测特征(predictor)，如果分别用”元”和”周岁”量纲来记录这两个预测特征的数据，则前者该列的数据的平均值在几千甚至几万这个水平，数据跨度可能是4000~40000;而后者的平均值一般在几十这个水平，数据跨度可能是20～100.两厢比较，在特征空间来看工资和年龄这两个维度的话，前者的平均值远远大于后者，而且前者分布的方差远远大于后者的方差，这种预测特征的平均值和方差差异可能会造成模型的性能减弱，导致模型不稳定</p>
<h2 id="处理方法"><a href="#处理方法" class="headerlink" title="处理方法"></a>处理方法</h2><p>标准化(standardization)：centering +  scaling;<br>其中centering是指预测特征的每个值减去该预测特征列的平均值;scaling是指预测特征的每个值除以该预测特征列的方差</p>
<h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p>能够提升计算上的数值稳定性，改善模型(比如偏线性回归)的性能，但是scale操作改变了预测特征原来的量纲，因此会让模型的可解释性变差。</p>
<h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(data.table)</div><div class="line"><span class="comment"># 将adult训练数据集加载到R的工作环境中</span></div><div class="line">data &lt;- as.data.frame(fread(<span class="string">'./adult.data'</span>,na.strings=<span class="string">'?'</span>,stringsAsFactors=<span class="literal">T</span>))</div><div class="line"><span class="comment"># 增加每一列的列名</span></div><div class="line">colnames(data) &lt;- c(<span class="string">"age"</span>,<span class="string">"workclass"</span>,<span class="string">"fnlwgt"</span>,<span class="string">"education"</span>,<span class="string">"education.num"</span>,<span class="string">"marital.status"</span>,<span class="string">"occupation"</span>,</div><div class="line">		<span class="string">"relationship"</span>,<span class="string">"race"</span>,<span class="string">"sex"</span>,<span class="string">"capital.gain"</span>,<span class="string">"capital.loss"</span>,<span class="string">"hours.per.week"</span>,<span class="string">"native.country"</span>,<span class="string">"output"</span>)</div><div class="line"><span class="comment"># 划分为训练数据和输出类别两部分</span></div><div class="line">train_output &lt;- data[[<span class="string">'output'</span>]]</div><div class="line">train_data &lt;- data</div><div class="line">train_data$output &lt;- <span class="literal">NULL</span></div><div class="line"></div><div class="line"><span class="comment"># 区分数值型特征和类别型特征</span></div><div class="line">col_numeric &lt;- colnames(train_data)[sapply(train_data, is.numeric)]</div><div class="line">col_category &lt;- colnames(train_data)[sapply(train_data, is.factor)]</div><div class="line">&gt; col_numeric</div><div class="line">[<span class="number">1</span>] <span class="string">"age"</span>            <span class="string">"fnlwgt"</span>         <span class="string">"education.num"</span>  <span class="string">"capital.gain"</span>  </div><div class="line">[<span class="number">5</span>] <span class="string">"capital.loss"</span>   <span class="string">"hours.per.week"</span></div><div class="line">&gt; col_category</div><div class="line">[<span class="number">1</span>] <span class="string">"workclass"</span>      <span class="string">"education"</span>      <span class="string">"marital.status"</span> <span class="string">"occupation"</span>    </div><div class="line">[<span class="number">5</span>] <span class="string">"relationship"</span>   <span class="string">"race"</span>           <span class="string">"sex"</span>            <span class="string">"native.country"</span></div><div class="line"></div><div class="line"><span class="comment"># 查看每一列的大致分布情况，注意类别特征必须是因子类型才能够进行统计</span></div><div class="line">&gt; summary(train_data)</div><div class="line">      age                   workclass         fnlwgt       </div><div class="line"> Min.   :<span class="number">17.00</span>   Private         :<span class="number">22696</span>   Min.   :  <span class="number">12285</span>  </div><div class="line"> 1st Qu.:<span class="number">28.00</span>   Self-emp-not-inc: <span class="number">2541</span>   1st Qu.: <span class="number">117827</span>  </div><div class="line"> Median :<span class="number">37.00</span>   Local-gov       : <span class="number">2093</span>   Median : <span class="number">178356</span>  </div><div class="line"> Mean   :<span class="number">38.58</span>   <span class="literal">NA</span>              : <span class="number">1836</span>   Mean   : <span class="number">189778</span>  </div><div class="line"> 3rd Qu.:<span class="number">48.00</span>   State-gov       : <span class="number">1298</span>   3rd Qu.: <span class="number">237051</span>  </div><div class="line"> Max.   :<span class="number">90.00</span>   Self-emp-inc    : <span class="number">1116</span>   Max.   :<span class="number">1484705</span>  </div><div class="line">                 (Other)         :  <span class="number">981</span>                    </div><div class="line">        education     education.num                 marital.status </div><div class="line"> HS-grad     :<span class="number">10501</span>   Min.   : <span class="number">1.00</span>   Divorced             : <span class="number">4443</span>  </div><div class="line"> Some-college: <span class="number">7291</span>   1st Qu.: <span class="number">9.00</span>   Married-AF-spouse    :   <span class="number">23</span>  </div><div class="line"> Bachelors   : <span class="number">5355</span>   Median :<span class="number">10.00</span>   Married-civ-spouse   :<span class="number">14976</span>  </div><div class="line"> Masters     : <span class="number">1723</span>   Mean   :<span class="number">10.08</span>   Married-spouse-absent:  <span class="number">418</span>  </div><div class="line"> Assoc-voc   : <span class="number">1382</span>   3rd Qu.:<span class="number">12.00</span>   Never-married        :<span class="number">10683</span>  </div><div class="line"> 11th        : <span class="number">1175</span>   Max.   :<span class="number">16.00</span>   Separated            : <span class="number">1025</span>  </div><div class="line"> (Other)     : <span class="number">5134</span>                   Widowed              :  <span class="number">993</span>  </div><div class="line">           occupation           relationship                   race      </div><div class="line"> Prof-specialty :<span class="number">4140</span>   Husband       :<span class="number">13193</span>   Amer-Indian-Eskimo:  <span class="number">311</span>  </div><div class="line"> Craft-repair   :<span class="number">4099</span>   Not-<span class="keyword">in</span>-family : <span class="number">8305</span>   Asian-Pac-Islander: <span class="number">1039</span>  </div><div class="line"> Exec-managerial:<span class="number">4066</span>   Other-relative:  <span class="number">981</span>   Black             : <span class="number">3124</span>  </div><div class="line"> Adm-clerical   :<span class="number">3770</span>   Own-child     : <span class="number">5068</span>   Other             :  <span class="number">271</span>  </div><div class="line"> Sales          :<span class="number">3650</span>   Unmarried     : <span class="number">3446</span>   White             :<span class="number">27816</span>  </div><div class="line"> Other-service  :<span class="number">3295</span>   Wife          : <span class="number">1568</span>                             </div><div class="line"> (Other)        :<span class="number">9541</span>                                                    </div><div class="line">     sex         capital.gain    capital.loss    hours.per.week </div><div class="line"> Female:<span class="number">10771</span>   Min.   :    <span class="number">0</span>   Min.   :   <span class="number">0.0</span>   Min.   : <span class="number">1.00</span>  </div><div class="line"> Male  :<span class="number">21790</span>   1st Qu.:    <span class="number">0</span>   1st Qu.:   <span class="number">0.0</span>   1st Qu.:<span class="number">40.00</span>  </div><div class="line">                Median :    <span class="number">0</span>   Median :   <span class="number">0.0</span>   Median :<span class="number">40.00</span>  </div><div class="line">                Mean   : <span class="number">1078</span>   Mean   :  <span class="number">87.3</span>   Mean   :<span class="number">40.44</span>  </div><div class="line">                3rd Qu.:    <span class="number">0</span>   3rd Qu.:   <span class="number">0.0</span>   3rd Qu.:<span class="number">45.00</span>  </div><div class="line">                Max.   :<span class="number">99999</span>   Max.   :<span class="number">4356.0</span>   Max.   :<span class="number">99.00</span>  </div><div class="line">                                                                </div><div class="line">       native.country </div><div class="line"> United-States:<span class="number">29170</span>  </div><div class="line"> Mexico       :  <span class="number">643</span>  </div><div class="line"> <span class="literal">NA</span>           :  <span class="number">583</span>  </div><div class="line"> Philippines  :  <span class="number">198</span>  </div><div class="line"> Germany      :  <span class="number">137</span>  </div><div class="line"> Canada       :  <span class="number">121</span>  </div><div class="line"> (Other)      : <span class="number">1709</span></div></pre></td></tr></table></figure>
<p>从结果可以看到，6个数值型特征中，”age”,”education.num”,”hours.per.week”这三个特征的平均分布水平与”fnlwgt”的平均分布相差巨大，另外后者与另外的两个特征”capital.loss”,”capital.gain”的分布离散程度远远大于前三个特征，因此可能在建模之前需要对这六个数值型特征进行标准化处理。</p>
<h1 id="有偏数据的检测和处理"><a href="#有偏数据的检测和处理" class="headerlink" title="有偏数据的检测和处理"></a>有偏数据的检测和处理</h1><h2 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a>问题描述</h2><p>无偏的预测特征是指其取值的分布是大致对称的，一种理想的无偏分布的例子就是正态分布。与无偏相反的有偏有两种情况：大部分值集中在数值偏小的一端，只有小部分值分布在数值偏大的一端，称为正偏（右偏）;与之相反的分布称为负偏（左偏）。</p>
<h2 id="检测与处理方法"><a href="#检测与处理方法" class="headerlink" title="检测与处理方法"></a>检测与处理方法</h2><p>一种简单的检测有偏的方法是查看该预测特征列的直方图，可以通过调整组距来探寻数据分布更多的细节信息，但是这种方法在准确度和效率方面较差。另一种方法是采用统计上的偏度值统计量来量化偏度的程度，偏度值越接近0,分布越对称;偏度值越大于0,正偏越严重;偏度值越小于0,负偏越严重。</p>
<p>两种处理方法：BoxCox转换，只适用于正数的预测特征列值; Yeo-Johnson方法，能够适用于一般的数值列</p>
<h2 id="例子-1"><a href="#例子-1" class="headerlink" title="例子"></a>例子</h2><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(e1071)</div><div class="line">skewValues &lt;- apply(train_data[col_numeric], <span class="number">2</span>, skewness)</div><div class="line">&gt; skewValues</div><div class="line">           age         fnlwgt  education.num   capital.gain   capital.loss    hours.per.week </div><div class="line">     <span class="number">0.5586919</span>      <span class="number">1.4468468</span>     -<span class="number">0.3116472</span>     <span class="number">11.9527463</span>      <span class="number">4.5942058</span>         <span class="number">0.2276216</span></div></pre></td></tr></table></figure>
<p>可以看到，”fnlwgt”存在轻度正偏，”capital.gain”和”capital.loss”的正偏比较严重，其余三个数值型特征的分布偏度较弱<br><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(car)</div><div class="line">yj_data &lt;- yjPower(train_data[[<span class="string">'capital.gain'</span>]],lambda=<span class="number">0</span>)  <span class="comment">#lambda一般取值[-2, 2] </span></div><div class="line">new_skew &lt;- skewness(yj_data)</div><div class="line">&gt; new_skew</div><div class="line">[<span class="number">1</span>] <span class="number">3.095858</span></div></pre></td></tr></table></figure></p>
<p>我们选择偏度最严重的”capital.gain”进行去偏处理，因为该特征取值包含了0，而BoxCox要求转换的值列大于0，<br>因此只能用Yeo-Johnson方法。处理后的新的特征”yj_data”的偏度严重减弱不少。</p>
<h1 id="离群点的检测和处理"><a href="#离群点的检测和处理" class="headerlink" title="离群点的检测和处理"></a>离群点的检测和处理</h1><h2 id="问题描述-2"><a href="#问题描述-2" class="headerlink" title="问题描述"></a>问题描述</h2><p>离群点的非正式定义是：datas that exceptionally far from the mainstream of the data[1]。一般需要针对具体的数据和任务对其中的”exceptionally”和”mainstream”进行界定。一些很明显的scientifically unvalid的预测特征值(比如年龄为负值)可以直接删除。在一些数据样本数量不够大且总体分布本身有偏时，处于尾部的疑似离群点的样本点可能也是来自于该总体分布的tail部分，这是因为总体分布的尾部部分采集样本时被采集到的概率很小，如果采样次数太少，则有可能来自于tail部分的样本点会比较少，从而看起来像是离群点，对于这些样本不应该删除，而是减弱它们的影响同时参与模型的构建。</p>
<h2 id="检测方法"><a href="#检测方法" class="headerlink" title="检测方法"></a>检测方法</h2><p>最基本的离群点侦测方法包括利用直方图、箱线图、DBSCAN聚类来定位可能的离群点。箱线图假设数据符合高斯分布，通过计算$Q_3 + 1.5 \times IQR$和$Q_1 - 1.5 \times IQR$(其中$IQR = Q_3 - Q_1$表示第三个四分位点与第一个四分位点的差)，大于前者或者小于后者的数据均被列为可能的离群点，如果用在不同类中，则为了得到离群点的排名，需要去除不同类的分布差异所带来的影响，因此除以对应类下的IQR值。箱线图中定位离群点时选择了分位点来衡量属性的分布集中情况(第二个四分位点，即中位数)和散步情况(IQR)，是因为它们相对于平均值和方差来说对于异常点具有较好的鲁棒性。</p>
<p>上面所说的基本的离群点的检测方法只能够用在特征较少的情况下。在特征数较大的数据中常用的离群点的检测方法包括Robust Covariance、One_Class SVM、局部离群点检测LOF算法以及Isolation Forest。</p>
<h3 id="Robust-Convariance"><a href="#Robust-Convariance" class="headerlink" title="Robust Convariance"></a>Robust Convariance</h3><p>这种检测异常点的大概思路是通过一种具备鲁棒性的方式求出符合inlier数据分布的参数，然后计算所有样本点到该分布的Mahalanobis距离，作为异常点的衡量标准。Robust Convariance方法在高维特征的情况下效果会变差，实际应用中一般需要满足：样本数大于两倍的特征数。scikit-learn实现了高斯分布情况下的Robust Convariance方法：sklearn.covariance.EllipticEnvelope。</p>
<h3 id="One-Class-SVM"><a href="#One-Class-SVM" class="headerlink" title="One_Class SVM"></a>One_Class SVM</h3><p>严格来说One_class SVM属于Novelty Detection(异常点检测：训练集不包含离群点，检测一个新的样本是否异常)。该方法在embedding的p-维空间中学习一个闭环边界，确定一个包含初始样本点(不含离群点)的子空间，对于新的样本确定其是否在这个闭环所包的子空间内，如果在，则被认为是与初始样本集同分布的;否则，如果落在子空间外面，则我们以一定的概率认为它是异常点。sklearn.svm.OneClassSVM实现这一异常点检测算法。</p>
<h3 id="Isolation-Forest"><a href="#Isolation-Forest" class="headerlink" title="Isolation Forest"></a>Isolation Forest</h3><p>孤立点检测算法iForest跟Random Forest类似，同样是由多颗子树iTree组成的森林，不同的是iTree的构建方法不同。iTree构建时从$n$个样本的数据集中均匀采样$\psi$个样本，在这个子数据集中，每次随机的选择一个属性、在该属性取值的$[min, max]$范围内随机取值分别作为切分变量和切分点，据此对$\psi$个样本划分到左右结点中：小于切分点的样本放入左结点，大于切分点的样本放入右结点。在左右结点中重复上述过程，直到满足终止条件(样本不可分或者iTree树的高度超过$\log \psi$).按照上面的步骤建好了$t$颗树之后，怎么判断一个样本是否有可能是异常点呢？对每个样本$x$计算它的anomaly score,这个得分的取值范围在$[0,1]$中，得分越靠近1,越有可能这个样本是异常点，样本x的anomaly score的具体计算公式为：<br>$$
s(x)=2^{-\frac{E[h(x)]}{c(\psi)}}
$$<br>其中$h(x)$是样本x在第h棵树中检索到的结点的高度，$E[\cdot]$是对样本x在全部t棵树中检索到高度求平均值;$c(\psi)$是在$\psi$个样本上构建的t颗树的平均路径长度。在高维属性情况下，每个iTree分别采用Kurtosis test(峰度：描述属性取值分布形态的陡缓程度的统计量)对属性进行筛选，只考虑那些容易引起空间位置突变的属性(文本分类时还可以采用卡方检验，结合那些对分类有意义的特征[9])。</p>
<p>在应用上，可以用于欺诈检测，及时发现异常交易、异常用户和异常信息流，另外也可以用在为文本分类器剔除错误标记的语料，为聚类任务去噪等[9]。</p>
<h4 id="iForest例子"><a href="#iForest例子" class="headerlink" title="iForest例子"></a>iForest例子</h4><p>选择UCI中的SMS Spam Collection[10]数据集，该数据集包括5574条文本，分两类，747条文本为spam文本(垃圾文本)，剩余的4827条文本为ham文本，即正常的文本，我希望iForest算法能够以较高的召回率找出这些spam文本。</p>
<p>为了方便后面load_files函数将各条文本即对应的类别读取到Bunch对象中进行TF-IDF处理，需要先将从UCI[10]下载的文件进行处理，原始数据的每一行代表一条文本以及对应的类别，中间以制表符分隔，制表符前面是类别，后面是对应的文本。我们将属于spam类的行保存为spam目录下的单个的文本，将属于ham类的行保存为ham目录下的单个文本：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">dir_str = <span class="string">'/mnt/shared/Dataset/smsspamcollection'</span></div><div class="line">f = open(dir_str+<span class="string">'/SMSSpamCollection'</span>,<span class="string">'r'</span>)</div><div class="line">i = <span class="number">0</span></div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</div><div class="line">    split = re.split(<span class="string">r'\t'</span>, line)</div><div class="line">    <span class="keyword">if</span> <span class="string">'spam'</span> == split[<span class="number">0</span>]:</div><div class="line">        <span class="keyword">with</span> open(dir_str+<span class="string">'/spam/'</span>+bytes(i)+<span class="string">'.txt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> write_spam:</div><div class="line">            write_spam.write(split[<span class="number">1</span>])</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">with</span> open(dir_str+<span class="string">'/ham/'</span>+bytes(i)+<span class="string">'.txt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> write_ham:</div><div class="line">            write_ham.write(split[<span class="number">1</span>])</div><div class="line">    i += <span class="number">1</span></div><div class="line">f.close()</div><div class="line"><span class="keyword">print</span> <span class="string">'split done!'</span></div></pre></td></tr></table></figure></p>
<p>re.split函数用于对每一行文本根据制表符进行切分，第一部分为类别，第二部分为文本正文。对原始的数据处理完之后，需要将每个文本转换为TF-IDF的向量表示形式，在此基础上再利用iForest进行异常样本检测：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_files</div><div class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</div><div class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfTransformer</div><div class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> IsolationForest</div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</div><div class="line"></div><div class="line"><span class="comment"># 将sms spam文本转换为TF-IDF矩阵形式</span></div><div class="line">dir_str = <span class="string">'/mnt/shared/Dataset/smsspamcollection/'</span></div><div class="line">spamdata = load_files(dir_str+<span class="string">'splited/'</span>,</div><div class="line">                  categories = <span class="keyword">None</span>,</div><div class="line">                  shuffle = <span class="keyword">True</span>,</div><div class="line">                  random_state = <span class="number">42</span>)</div><div class="line">count_vect = CountVectorizer(spamdata.data, stop_words = <span class="string">'english'</span>, min_df=<span class="number">20</span>, max_df=<span class="number">0.85</span>)</div><div class="line">train_counts = count_vect.fit_transform(spamdata.data)</div><div class="line">train_tfidf = TfidfTransformer().fit_transform(train_counts)</div><div class="line"></div><div class="line"><span class="comment"># 在TF-IDF形式的训练集上训练iForest模型并对训练集数据进行预测</span></div><div class="line">clf = IsolationForest(n_estimators=<span class="number">300</span>, max_samples=<span class="number">200</span>, contamination=<span class="number">0.1</span>)</div><div class="line">clf.fit(train_tfidf)</div><div class="line">outlier = clf.predict(train_tfidf)</div><div class="line"></div><div class="line"><span class="comment"># 将预测结果进行转换方便打印混淆矩阵</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, outlier.shape[<span class="number">0</span>]):</div><div class="line">    <span class="keyword">if</span> outlier[i] == <span class="number">1</span>:</div><div class="line">        outlier[i] = <span class="number">0</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        outlier[i] = <span class="number">1</span></div><div class="line">print(<span class="string">'Print Confusion Matrix'</span>)</div><div class="line">print(metrics.classification_report(spamdata.target, outlier,target_names=spamdata.target_names))</div><div class="line"></div><div class="line">&gt;&gt;&gt;&gt;&gt;</div><div class="line">Print Confusion Matrix</div><div class="line">             precision    recall  f1-score   support</div><div class="line"></div><div class="line">        ham       <span class="number">0.92</span>      <span class="number">0.96</span>      <span class="number">0.94</span>      <span class="number">4827</span></div><div class="line">       spam       <span class="number">0.64</span>      <span class="number">0.48</span>      <span class="number">0.55</span>       <span class="number">747</span></div><div class="line"></div><div class="line">avg / total       <span class="number">0.88</span>      <span class="number">0.89</span>      <span class="number">0.89</span>      <span class="number">5574</span></div></pre></td></tr></table></figure></p>
<p>上面代码中有几个地方说明一下：CountVectorizer()函数中的参数stop_words=’english’表示采取scikit-learn中内置的英文停用词表对文本进行停用词过滤。实际上因为我们选择的sms spam collection数据集中的文本大部分(4827/5574)是人们之间正常沟通的短信，因此里面包含了大量的口语和具有个人风格的缩略词，下面是一部分文本样本：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">ham	Is that seriously how you spell his name?</div><div class="line">ham	I‘m going to try for 2 months ha ha only joking</div><div class="line">ham	So ü pay first lar... Then when is da stock comin...</div><div class="line">ham	Aft i finish my lunch then i go str down lor. Ard 3 smth lor. U finish ur lunch already?</div><div class="line">ham	Ffffffffff. Alright no way I can meet up with you sooner?</div></pre></td></tr></table></figure></p>
<p>因此设置min_df=20表示如果一个词在少于20个文本中出现，则过滤掉这个词;max_df=0.85表示如果一个词在超过85%的文本中出现过，则被认为不具有区分不同类别文本的信息，同样过滤掉。</p>
<p>iForest对训练集预测得到的原始outlier值只包括1和-1两个取值，1代表iForest对该文本预测为非异常的，-1则代表iForest认为该文本是异常的。事实上，iForest对于每个文本样本计算的原始的normal score值可以通过clf.decision_function(train_tfidf)得到，这个normal score值的取值在[0,1]区间，那么predict()函数根据什么来判断一个样本是否是异常的呢？很显然需要一个判断normal score的阈值来进行比较。这个阈值的计算通过IsolationForest()函数的contamination参数确定，这个参数指定的是数据集中异常点的比例，从而通过这个比例值确定decision_function()返回的normal score列表中对应的百分位值，即为比较的阈值，normal score值小于这个阈值的样本被标记为-1，对应spam类，通过查看spamdata.target_names可得知spam在下标为1的位置，ham在下标为0的位置。因此在上面的代码中将iForest的预测值还原为可以跟真实的类别spamdata.target进行比较的形式。下面的代码来自于IsolationForest的scikit-learn实现，是计算阈值和输出预测值1和-1的关键部分：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># from fit() function</span></div><div class="line">self.threshold_ = -sp.stats.scoreatpercentile(</div><div class="line">            -self.decision_function(X), <span class="number">100.</span> * (<span class="number">1.</span> - self.contamination))</div><div class="line"><span class="comment"># from predict() function</span></div><div class="line">is_inlier = np.ones(X.shape[<span class="number">0</span>], dtype=int)</div><div class="line">is_inlier[self.decision_function(X) &lt;= self.threshold_] = <span class="number">-1</span></div></pre></td></tr></table></figure></p>
<p>所以增大参数contamination的值能够提高spam类的召回率，但是精确率会下降，下面的结果是contamination取0.3的结果：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Print Confusion Matrix</div><div class="line">             precision    recall  f1-score   support</div><div class="line"></div><div class="line">        ham       <span class="number">0.96</span>      <span class="number">0.77</span>      <span class="number">0.85</span>      <span class="number">4827</span></div><div class="line">       spam       <span class="number">0.34</span>      <span class="number">0.77</span>      <span class="number">0.48</span>       <span class="number">747</span></div><div class="line"></div><div class="line">avg / total       <span class="number">0.87</span>      <span class="number">0.77</span>      <span class="number">0.80</span>      <span class="number">5574</span></div></pre></td></tr></table></figure></p>
<p>我们看一下iForest对于两个类别的文本预测的normal score的分布范围，scikit-learn中的decision_function用于计算样本的normal score值，这个值越低表示该样本越异常，代码和对应的值分布图如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line">clf.fit(train_tfidf)</div><div class="line">score = clf.decision_function(train_tfidf)</div><div class="line">spam_index = [i <span class="keyword">for</span> i,x <span class="keyword">in</span> enumerate(spamdata.target) <span class="keyword">if</span> <span class="number">1</span>==x]</div><div class="line">ham_index = [i <span class="keyword">for</span> i,x <span class="keyword">in</span> enumerate(spamdata.target) <span class="keyword">if</span> <span class="number">0</span>==x]</div><div class="line">spam_score = score[spam_index]</div><div class="line">ham_score = score[ham_index]</div><div class="line">plt.hist(spam_score,facecolor=<span class="string">'green'</span>,alpha=<span class="number">0.5</span>,label=<span class="string">'spam'</span>)</div><div class="line">plt.hist(ham_score,facecolor=<span class="string">'blue'</span>,alpha=<span class="number">0.5</span>,label=<span class="string">'ham'</span>)</div><div class="line">plt.xlabel(<span class="string">'score of spam text'</span>)</div><div class="line">plt.ylabel(<span class="string">'Count'</span>)</div><div class="line">plt.legend(loc=<span class="string">'upper left'</span>)</div><div class="line">plt.grid(<span class="keyword">True</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure></p>
<p><img src="http://onvolufm1.bkt.clouddn.com/smsspam_iForest.png" alt=""><br>从normal score值的分布来看，iForest并没有很好的把spam类文本和ham类文本区分开来，中间存在较大的重叠部分，从而在我们增大参数contamination的值使得spam类的召回率改善时会让spam类的精确率极低，因为此时重叠部分大量的ham类样本被错分为spam类。</p>
<h3 id="Local-Outlier-Factor"><a href="#Local-Outlier-Factor" class="headerlink" title="Local Outlier Factor"></a>Local Outlier Factor</h3><p>LOF算法是基于密度的离群点检测算法，它基于这样一个观察：The density around an outlier object is significantly different from the density around its neighbors[11]，即离群点附近的密度与其邻居的邻近密度差距很大。因此LOF计算一个数据点对它的邻居数据点的相对密度(LOF值)来作为它是否为离群点的度量方式，这个相对密度越大，说明该数据点越有可能是离群点。</p>
<p>LOF算法描述如下：<br>输入：输入数据D，k(选择的邻居数)，距离度量方式T；<br>输出：根据LOF值从大到小排序的样本标号；<br>过程：</p>
<ol>
<li>根据距离度量方式T计算D中两两数据点之间的距离$dist(d_i, d_j)$;</li>
<li>根据设定的k值计算数据点$o$的k-distance:$dist_k (o)$，这是该数据点到其第k个最近邻居之间的距离；</li>
<li>计算数据点$o$的k-distance邻居$N_k (0)=\{ \hat o | \hat o in D, dist(o,\hat o) \le dist_k(o)\}$，即找到那些距离数据点不大于其与第k个最近邻的距离的其他数据点(可能不止k个，因为其中可能有多个数据点到o的距离相同)；</li>
<li>计算数据点$o$的local reachability density: $lrd_k (o)={\|N_k(o) \|} \over {\sum_{\hat o \in N_k(o)} reachdist_k (\hat o \leftarrow o)}$，其中分母部分称为从$\hat o$到$o$的reachability distance：$max \{dist_k(o), dist(o,\hat o) \}$;</li>
<li>计算数据点$o$的LOF值：$LOF_k(o)={{\sum_{\hat o \in N_k(o) {{lrd_k(\hat o)} \over {lrd_k(o)}}} \over {\| N_k(o)\|}}$，可以看到数据点$o$LOF值的计算实际为它的k-distance邻居的local reachability对$o$的比值的平均值；</li>
<li>对于数据集D中的所有数据点重复过程2-5,然后根据LOF值从大到小对数据点进行排序输出。</li>
</ol>
<h2 id="处理方法-1"><a href="#处理方法-1" class="headerlink" title="处理方法"></a>处理方法</h2><p>显式的离群点处理方法spatial sign，它的思想是对于<strong>样本</strong>的每一个属性除以该样本在P-维属性空间中距离原点的欧式距离，公式如下。几何上来看，相当于把所有的样本点映射到p-维属性空间的单位球面上，缓解离群点的影响。<br>$$
x_{ij}^\star = {x_{ij} \over \sqrt{\sum_{j=1}^P x_{ij}^2}}
\tag{0}
$$</p>
<p>某些模型本身具有对离群点较好的鲁棒性，包括基于回归树的模型、软间隔SVM(只选择少数的支撑向量建模)、逻辑回归模型(sigmoid非线性变换缓解离群点的影响)。</p>
<h1 id="降维和特征选择"><a href="#降维和特征选择" class="headerlink" title="降维和特征选择"></a>降维和特征选择</h1><h2 id="问题描述-3"><a href="#问题描述-3" class="headerlink" title="问题描述"></a>问题描述</h2><p>降维和特征选择是有区别的，降维是指将p-维的特征空间减小到k-维同时尽量保留原来特征空间的某种信息，而特征选择是指去除一些无用的或者信息量太少的特征，保留具有高信息量的强特征。特征选择往往会造成降维的效果，但是降维不意味着进行了特征选择。</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h3><h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>PCA是降维方法，严格来说不具有特征选择的作用，因为它的主要思想是通过原始的预测特征的线性组合构造新的主成分)，构造的原则是找到一组两两正交的方向形成一个新的特征坐标系，然后将原始的特征向量在这个新的坐标系下进行投影，从而得到一组新的主成分，如果新的坐标系的维度小于原始的特征空间坐标系，则此时PCA具有降维的作用。关键就在于如何找到这一组两两正交的方向。</p>
<p>PCA寻找这组方向的标准是在与已经找到的方向正交的前提下尽可能多的capture到原始预测特征中的方差(variability)，示意图[2]如下：<br><img src="http://onvolufm1.bkt.clouddn.com/PCA.png" alt=""><br>可以看到，The largest principal component is the direction that maximazes the variance of the projected data[2].为了寻找到这样一组方向，需要借助SVD分解。</p>
<p>SVD(奇异值分解)是将一个矩阵分解为其他三个矩阵相乘的形式，核心表达式为：</p>
<script type="math/tex; mode=display">
A_{m \times n} = U_{m \times m} D_{m \times n} V_{n \times n}^T
\tag{1}</script><p>其中矩阵$U,V$的列向量分别由下面两个正定矩阵的特征向量组成：</p>
<script type="math/tex; mode=display">
A_{m \times n} A_{n \times m}^T,\qquad A_{n \times m}^T A_{m \times n}
\tag{2}</script><p>这里重要的一点是这些特征向量在$U,V$中的排列顺序，它们的排列顺序取决于对应特征值的大小。对应(2)中的两个正定矩阵，分别存在一组特征值$\lambda_1^1,\lambda_2^1,\ldots,\lambda_m^1$和$\lambda_1^2,\lambda_2^2,\ldots,\lambda_n^2$，如果将这两组特征值从大到小排序，则前$min(m,n)$个特征值是一样的且均大于零，后面的$max(m,n)-min(m,n)$个特征值均为0,因此只需要按照其中一组特征值的大小来排列特征向量在$U,V$中的顺序。剩下的矩阵$D_{m \times n}$的主对角线上的值就是从大到小排列后的$min(m,n)$个特征值的平方根(称为A的奇异值)。</p>
<p>那么如何从SVD联系到PCA呢？按照奇异值从大到小的顺序我们取前$r \lt min(m,n)$个奇异值和对应的奇异向量，根据式(1)有下面的等式近似成立：</p>
<script type="math/tex; mode=display">
A_{m \times n} \approx U_{m \times r} D_{r \times r} V_{r \times n}^T
\tag{3}</script><p>约等式两边同时右乘$V_{n \times r}$，又因为$V_{r \times n}^T V_{n \times r} = I_{r \times r}$，因此有：</p>
<script type="math/tex; mode=display">
A_{m \times r}^{pca} = A_{m \times n} V_{n \times r} \approx U_{m \times r} D_{r \times r}
\tag{4}</script><p>如果$A_{m \times n}$表示有m个样本n个预测特征的数据集，则通过式(4)的变换将预测特征空间的维度从n减小到r。r的选择可以通过交叉验证来实现或者通过scree plot来选择。</p>
<p>对比PCA的降维思想和SVD，可以发现，PCA所希望寻找那一组两两正交的方向就是A矩阵的一组右奇异向量，而A的右奇异值用来衡量原始数据集在对应的右奇异向量上的方差。</p>
<h4 id="优劣势"><a href="#优劣势" class="headerlink" title="优劣势"></a>优劣势</h4><p>PCA创建的主成分特征之间是线性无关的，适合于对特征弱相关性有要求的学习模型(比如朴素贝叶斯、线性回归模型)；同时PCA也能够用于去除数据集中无关的冗余信息，在图像存储中可以用较小的存储空间来保存图片同时保留较好的图像还原度。</p>
<p>从PCA降维的过程可以发现，PCA是个无监督的降维过程，也就是说它在构建这些主成分的时候并没有考虑到它们与输出变量之间的关系;另外主成分的构建标准是投影方差最大化，但是一个预测特征所包含的信息可能无法通过方差来很好的衡量，如果一个与输出变量关系不大的预测特征的方差非常大，则经过PCA降维处理后，该预测特征可能会在多个主成分中占据较大的分量(loading)，为了克服这种情况的出现，最好对有偏的预测特征进行去偏处理，并对所有的特征进行标准化处理。关于PCA更具体和详细的分析和推导参见参考文献[4].</p>
<h3 id="PLS"><a href="#PLS" class="headerlink" title="PLS"></a>PLS</h3><p>偏最小二乘跟PCA类似的地方在于它所寻找的PLS方向同样是原来预测特征的线性组合，不同的地方在于确定PLS方向时的准则既做到尽可能的提取原来预测特征的方差信息，又做到所确定的PLS方向是与输出变量的相关性最大。同样，PLS需要对数据集(包括输出变量)进行标准化处理，具体的算法步骤参考了文献[3],下面对算法过程进行简单梳理。</p>
<ul>
<li>输入：数据集预测变量部分$E_0$,数据集输出变量部分$Y$，假设包括m个样本，n个预测变量，则矩阵$E_0$维度为$m \times n$，只考虑一个输出变量，即$Y_{m \times 1}$，预测特征集$X=(x_1,\ldots,x_n)$.</li>
<li>输出：r个PLS成分$(t_1,t_2,\ldots,t_r)$</li>
</ul>
<ol>
<li>求矩阵$E_0^T Y Y^T E_0$最大特征值所对应的特征向量$w_1$,求得成分$t_1 = w_1^T X$,计算成分得分向量$\hat t_1 = E_0 w_1$和残差矩阵$E_1 = E_0 - \hat {t_1} \alpha_1^T$，其中$\alpha_1 = {{E_0^T \hat t_1} \over {\| \hat t_1 \|}^2}$;</li>
<li>求矩阵$E_1^T Y Y^T E_1$最大特征值所对应到特征向量$w_2$，求得成分$t_2 = w_2^T X$，计算成分得分向量$\hat t_2 = E_1 w_2$和残差矩阵$E_2 = E_1 - \hat {t_2} \alpha_2^T$，其中$\alpha_2 = {{E_1^T \hat t_2} \over {\| \hat t_2 \|}^2}$;</li>
<li>重复上述步骤，直到通过交叉有效性确定共抽取r个成分$t_1,t_2,\ldots,t_r$为止。</li>
</ol>
<h3 id="Fisher’s-Discriminant-Analysis"><a href="#Fisher’s-Discriminant-Analysis" class="headerlink" title="Fisher’s Discriminant Analysis"></a>Fisher’s Discriminant Analysis</h3><p>关于费雪判别分析的内容主要参考自[5]，本文只概述其原理和梳理其主要的过程，具体的数学推导在[5]中有详细和具体的论述。最原始的费雪判别分析是专用于二类别分类问题的降维方法，它希望能够找到一根过原点的直线，使得两个类别的数据样本在这条直线上的投影尽可能的分离开，同时使同一组内的投影点之间尽可能集中。为此，费雪提出的准则为：<br>$$
\max J(w) =\max {{m_2 - m_1}^2 \over {s_1^2 + s_2^2}}
$$<br>其中分子部分称为组间散布，用于衡量投影后两个类别样本之间的分离程度;分母部分称为整体组内散布，分别代表两个类别样本投影之后各类之间的集中程度，它们的定义分别为：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
m_j^{original} = {1 \over n_j} \sum_{i \in c_j} x_i, \quad ;
m_j = w^T m_j^{original}, \quad j=1,2; \\
s_j^2 = \sum_{i \in c_j}{(y_i - m_j)}^2 \\
\end{array}</script><p>将原始的$J(w)$进行改写后变为$J(w)=\frac{w^T S_B w}{w^T S_W w}$，此时最大化费雪准则等价与下列约束优化问题：</p>
<script type="math/tex; mode=display">
\max_{w^T S_W w = 1} w^T S_B w</script><p>通过求解这个优化问题得到唯一的最佳投影直线的方向向量为$w \propto S_W^{-1}(m_2 - m_1)$,其中：</p>
<script type="math/tex; mode=display">
S_W = \sum_{j=1,2} \sum_{i \in c_j} (x_i - m_j)(x_i - m_j)^T</script><p>上式表示整体组内散布矩阵。如果需要将费雪的判别分析引入分类功能，还需要在最佳投影直线上确定一个门槛值$w_0$来进行类别判断。在确定$w_0$时候，如果假设分类样本的条件概率$p(x | c_1),p(x | c_2)$服从正态分布并且两类别的数据点具有相同的散布形态(即具有相同的协方差矩阵)，则对应的分类方法称为线性判别分析LDA，关于门槛值$w_0$的具体求解过程在[5]中有详细的推导和说明。</p>
<p>前面所讲的费雪判别分析限定在二分类问题上，实际上可以将其推广到多分类的问题，具体的过程在[5]中末尾处有详细的推导，多分类下的费雪判别分析算法过程如下[6]:</p>
<ul>
<li>输入：N个D维向量$x_1,\ldots,x_N$，数据能够被分成d个类别;</li>
<li>输出：投影矩阵$W = (w_1,\ldots,w_{d-1})$,其中每一个$w_i$都是D维列向量</li>
</ul>
<ol>
<li>求出类内散度矩阵$S_w$和类间散度矩阵$S_b$;</li>
<li>对$S_w = UDV^T$做奇异值分解，求得$S_w^{-1} = VD^{-1}U^T$;</li>
<li>对矩阵$S_w^{-1} S_b$做特征值分解;</li>
<li>取最大的前d-1个特征值对应的特征向量$w_1,\ldots,w_{d-1}$.</li>
</ol>
<p>上述算法之所以选择取前面d-1个特征向量，是因为k类别判别分析所能产生的有效线性特征总数至多为k-1[5]。利用这k-1个D维特征向量，就可以得到k-1个新的预测特征值列。注意利用费雪判别分析做降维处理时不需要对数据进行去中心化，避免去中心化后每个类的中心会重叠[6].</p>
<h3 id="自编码器"><a href="#自编码器" class="headerlink" title="自编码器"></a>自编码器</h3><p>自编码器是一种特殊的神经网络，以单个隐藏层的自编码器为例，输出层尝试使得输出$\hat x$接近于输入$x$，从而通过隐藏层学习到输入数据的压缩表示，当给定一个数据的压缩表示，结合隐藏层和输出层之间的连接权重矩阵就可以重构出最原始未压缩的数据[7].为了学习到输入数据的压缩表示，隐藏层神经元个数要求小于输入层神经元个数，或者在隐藏层神经元个数大于输入层个数时加入稀疏性限制(稀疏自编码器)，从而学习到输入数据的低维结构。如果输入数据的维度较大或者希望找到数据维度更低的压缩表示，可以采用栈式自编码器，它以单隐藏层的自编码器为基础进行连接：前一个自编码器学习到的低维结构作为下一个自编码器的输入层。</p>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>其他的降维方法还包括多维缩放(MDS)(用于图片的降维[8])，基于流形学习的降维方法[6].</p>
<h2 id="例子-2"><a href="#例子-2" class="headerlink" title="例子"></a>例子</h2><p>我们利用PCA来对一张图片进行实验。<br><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(jpeg)</div><div class="line">greyflower &lt;- readJPEG(<span class="string">'greyflower.jpg'</span>)  <span class="comment"># 将灰度图像中像素点的值以矩阵的形式保存起来</span></div><div class="line">svdflower &lt;- svd(greyflower)  <span class="comment"># 对图像矩阵进行SVD分解</span></div><div class="line">sumD &lt;- sum((svdflower$d)^<span class="number">2</span>) <span class="comment"># 计算所有奇异值平方的总和</span></div><div class="line">percentD &lt;- (svdflower$d)^<span class="number">2</span>/sumD*<span class="number">100</span>  <span class="comment"># 求出每一个奇异值平方值在总和中所占的比例</span></div><div class="line">plot(x=c(<span class="number">1</span>:min(dim(greyflower))),y=percentD,</div><div class="line">        xlab=<span class="string">'主成分标号'</span>,ylab=<span class="string">'所占奇异值总和百分比(%)'</span>,type=<span class="string">'b'</span>,col=<span class="string">'red'</span>)</div></pre></td></tr></table></figure></p>
<p>这里的图片是灰度图，对其SVD分解后计算每个奇异值平方的占比，绘制陡坡图如下;如果是RGB彩色图，需要分别对三个颜色通道进行SVD分解，然后再还原为彩色图。<br><img src="http://onvolufm1.bkt.clouddn.com/PCA_scree_plot.png" alt=""><br>可以看到，原始图片的variability信息几乎可以完全被前面几个主成分解释。我们一次选择5、10、20、50、100和全部主成分用于灰度图还原：<br><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">singularNum &lt;- <span class="number">5</span>   <span class="comment"># 用于选择还原灰度图时的主成分个数</span></div><div class="line">greyflowerPca &lt;- svdflower$u[,c(<span class="number">1</span>:singularNum)]%*%</div><div class="line">        diag(svdflower$d[<span class="number">1</span>:singularNum])%*%t(svdflower$v[,c(<span class="number">1</span>:singularNum)])   <span class="comment"># 根据SVD表达式而来</span></div><div class="line">writeJPEG(greyflowerPca,target=<span class="string">'greyflowerPca.jpg'</span>,quality=<span class="number">1</span>)   <span class="comment"># 在当前工作目录下生成灰度图greyflowerPca.jpg</span></div></pre></td></tr></table></figure></p>
<p>得到的对比图如下：<br><img src="http://onvolufm1.bkt.clouddn.com/PCA_rebuildgrey.png" alt=""><br>我们对比一下用于还原灰度图的主成分的总特征值之和的占比分别是多少：<br><img src="http://onvolufm1.bkt.clouddn.com/PCA_ratio.png" alt=""></p>
<h1 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h1><p>缺失值如果集中在少数样本上或者少数预测特征上时，可以考虑将这些样本或者预测特征去掉，但是预测特征的移除需要谨慎考虑。更为一般的处理方法是进行缺失值的填充(imputation)，有两种常用的填充方法：</p>
<ol>
<li><p>基于<strong>样本</strong>的K-近邻方法<br>找到距离缺失值所在<strong>样本</strong>最近的k个邻居，利用这k个邻居的预测特征值的平均值或者加权平均值作为缺失值填充值。这种方法同样需要确定k的取值和选择距离的度量方式，好处是缺失值的填充值必定在该预测特征的range范围内。适用于缺失值集中在样本行上的数据集。</p>
</li>
<li><p>基于<strong>预测特征</strong>的局部建模方法<br>如果缺失值集中在某些预测特征A上，且存在其他预测特征B与A有线性相关关系或非线性的关系(可以从已有的数据判断或者从数据集的实际背景得到)，则以A为输出变量，以B为预测变量建立相应的拟合模型$f:B \rightarrow A$，利用建立的拟合模型$f$对A中的缺失值进行预测性填充。</p>
</li>
</ol>
<h2 id="例子-3"><a href="#例子-3" class="headerlink" title="例子"></a>例子</h2><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">col_NA &lt;- colnames(train_data)[apply(train_data,<span class="number">2</span>,anyNA)]</div><div class="line">num_NA &lt;- apply(train_data[col_NA],<span class="number">2</span>,<span class="keyword">function</span>(x)&#123;sum(is.na(x))&#125;)</div><div class="line">&gt; num_NA</div><div class="line">    workclass     occupation   native.country </div><div class="line">     <span class="number">1836</span>           <span class="number">1843</span>            <span class="number">583</span></div></pre></td></tr></table></figure>
<p>可以看到，所有的缺失值集中在以上三个类别型特征上，考虑到样本数(32561)较大，可以考虑对这些列进行值填充。caret包中的preProcess方法支持三种缺失值填充方法:基于KNN填充的、利用Bagging树填充的、利用中值填充的，我们选择用KNN填充作为例子,利用KNN进行填充时，它只会选择其中的数值型特征作为邻居的衡量方法。<br><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">knn_impute &lt;- preProcess(train_data, method=c(<span class="string">"knnImpute"</span>)</div><div class="line">new_train &lt;- predict(knn_impute, train_data)</div></pre></td></tr></table></figure></p>
<h1 id="预测特征的移除"><a href="#预测特征的移除" class="headerlink" title="预测特征的移除"></a>预测特征的移除</h1><h2 id="问题说明"><a href="#问题说明" class="headerlink" title="问题说明"></a>问题说明</h2><p>预测特征的移除一般是因为它没有提供有效的信息或者它提供了冗余的信息，而移除之后能够使得模型的训练更快，模型本身更简单，也能够提高模型的可解释性和稳定性。存在退化分布(degenerate distribution)的预测特征对于所有的样本提供了几乎相同的信息，因此对于模型而言没有提供具有区分度的信息，也就是说该预测特征没有提供有效信息；另一种情况是两个或者多个预测特征之间存在(共/多重共)线性关系时，相当于向模型提供了相同的信息，造成信息冗余，而且导致模型复杂度增加，稳定性变差，难以准确确定预测变量的相对重要性。下面分别对这两种情况的检测方法[1]进行说明。</p>
<h2 id="共线性检测"><a href="#共线性检测" class="headerlink" title="共线性检测"></a>共线性检测</h2><ol>
<li>VIF(variance inflation factor)方差膨胀因子只适用于线性回归，而且要求样本数大于预测变量的个数;</li>
<li>PCA主成分提取到的方差比例和对应的loading值(这个方法的说明后续补充);</li>
<li>预测变量之间的协相关系数矩阵：这是比较常用的方法</li>
</ol>
<p>在实际中用的比较多的是第三种方法，尽可能少的移除预测变量使得剩余的预测变量之间的相关性低于某个设定的阈值，步骤如下：</p>
<ol>
<li>计算所有预测变量之间的相关系数;</li>
<li>找出相关系数绝对值最大的系数所对应的两个预测变量A和B;</li>
<li>计算预测变量A与其他预测变量之间相关系数绝对值的平均值$\overline {cor}_A$,对B进行同样的计算得到$\overline {cor}_B$;</li>
<li>如果$\overline {cor}_A \gt \overline {cor}_B$，则移除预测变量A;否则移除B;</li>
<li>重复步骤2-4直到所有剩余预测变量之间的相关系数绝对值小于设定的阈值。</li>
</ol>
<h2 id="退化分布的预测特征的检测"><a href="#退化分布的预测特征的检测" class="headerlink" title="退化分布的预测特征的检测"></a>退化分布的预测特征的检测</h2><p>一个预测变量产生退化分布是指该预测变量样本取值相同或者绝大部分相同，前者称为Zero Variance Predictor，后者称为Near-zero Variance Predictor。</p>
<p>判断一个预测变量是否是Near-zero Variance Predictor，一般有两个条件：</p>
<ol>
<li>不同值的个数占所有取值个数的比例很低(比如10%);</li>
<li>出现次数最多的那个值的频数除以出现次数第二多的那个值的频数，这个比值很大(比如大于20)。 </li>
</ol>
<h2 id="例子-4"><a href="#例子-4" class="headerlink" title="例子"></a>例子</h2><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">index_col_category &lt;- sapply(col_category,simplify=<span class="literal">T</span>,<span class="keyword">function</span>(x)&#123;grep(x,colnames(train_data))&#125;)</div><div class="line">index_category &lt;- unlist(index_col_category,use.names=<span class="literal">F</span>)</div><div class="line">&gt; cor(train_data[-index_category])</div><div class="line">                       age        fnlwgt  capital.gain capital.loss</div><div class="line">age             <span class="number">1.00000000</span> -<span class="number">0.0766458679</span>  <span class="number">0.0776744982</span>   <span class="number">0.05777454</span></div><div class="line">fnlwgt         -<span class="number">0.07664587</span>  <span class="number">1.0000000000</span>  <span class="number">0.0004318858</span>  -<span class="number">0.01025171</span></div><div class="line">capital.gain    <span class="number">0.07767450</span>  <span class="number">0.0004318858</span>  <span class="number">1.0000000000</span>  -<span class="number">0.03161506</span></div><div class="line">capital.loss    <span class="number">0.05777454</span> -<span class="number">0.0102517117</span> -<span class="number">0.0316150630</span>   <span class="number">1.00000000</span></div><div class="line">hours.per.week  <span class="number">0.06875571</span> -<span class="number">0.0187684906</span>  <span class="number">0.0784086154</span>   <span class="number">0.05425636</span></div><div class="line">               hours.per.week</div><div class="line">age                <span class="number">0.06875571</span></div><div class="line">fnlwgt            -<span class="number">0.01876849</span></div><div class="line">capital.gain       <span class="number">0.07840862</span></div><div class="line">capital.loss       <span class="number">0.05425636</span></div><div class="line">hours.per.week     <span class="number">1.00000000</span></div></pre></td></tr></table></figure>
<p>上面的代码是查看数值型特征之间的相关性，从结果可以看出，基本上不存在线性相关关系，而且特征维度不高，因此没必要用PCA来构建线性无关的主成分特征。我们来检查一下是否存在呈现退化分布的特征。<br><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 10表示不同值对所有取值的占比阈值；30表示the ratio of the most common value to the second most common value</span></div><div class="line">nzv &lt;- nearZeroVar(train_data,uniqueCut=<span class="number">10</span>,freqCut=<span class="number">30</span>,saveMetrics=<span class="literal">T</span>)</div><div class="line">&gt; nzv</div><div class="line">                freqRatio percentUnique zeroVar   nzv</div><div class="line">age              <span class="number">1.011261</span>   <span class="number">0.224194589</span>   <span class="literal">FALSE</span> <span class="literal">FALSE</span></div><div class="line">workclass        <span class="number">8.931917</span>   <span class="number">0.024569270</span>   <span class="literal">FALSE</span> <span class="literal">FALSE</span></div><div class="line">fnlwgt           <span class="number">1.000000</span>  <span class="number">66.484444581</span>   <span class="literal">FALSE</span> <span class="literal">FALSE</span></div><div class="line">education        <span class="number">1.440269</span>   <span class="number">0.049138540</span>   <span class="literal">FALSE</span> <span class="literal">FALSE</span></div><div class="line">education.num    <span class="number">1.440269</span>   <span class="number">0.049138540</span>   <span class="literal">FALSE</span> <span class="literal">FALSE</span></div><div class="line">marital.status   <span class="number">1.401853</span>   <span class="number">0.021498111</span>   <span class="literal">FALSE</span> <span class="literal">FALSE</span></div><div class="line">occupation       <span class="number">1.010002</span>   <span class="number">0.042996222</span>   <span class="literal">FALSE</span> <span class="literal">FALSE</span></div><div class="line">relationship     <span class="number">1.588561</span>   <span class="number">0.018426952</span>   <span class="literal">FALSE</span> <span class="literal">FALSE</span></div><div class="line">race             <span class="number">8.903969</span>   <span class="number">0.015355794</span>   <span class="literal">FALSE</span> <span class="literal">FALSE</span></div><div class="line">sex              <span class="number">2.023025</span>   <span class="number">0.006142317</span>   <span class="literal">FALSE</span> <span class="literal">FALSE</span></div><div class="line">capital.gain    <span class="number">86.020173</span>   <span class="number">0.365467891</span>   <span class="literal">FALSE</span>  <span class="literal">TRUE</span></div><div class="line">capital.loss   <span class="number">153.673267</span>   <span class="number">0.282546605</span>   <span class="literal">FALSE</span>  <span class="literal">TRUE</span></div><div class="line">hours.per.week   <span class="number">5.398013</span>   <span class="number">0.288688922</span>   <span class="literal">FALSE</span> <span class="literal">FALSE</span></div><div class="line">native.country  <span class="number">45.365474</span>   <span class="number">0.125917509</span>   <span class="literal">FALSE</span>  <span class="literal">TRUE</span></div></pre></td></tr></table></figure></p>
<p>从结果可以看到，”capital.gain”,”capital.loss”,”native.country”这三个特征的freqRatio值非常大，意味着出现次数最多的取值远远大于出现次数第二多的取值，其中”capital.gain”,”capital.loss”的情况更加严重，我们观察一下这两个特征的直方图：<br><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(ggplot2)</div><div class="line"><span class="keyword">library</span>(gridExtra)</div><div class="line">p1 &lt;- ggplot(train_data,aes(capital.gain))+</div><div class="line">	  geom_histogram(aes(y=..density..))+</div><div class="line">	  geom_density(color=<span class="string">'red'</span>)</div><div class="line">p2 &lt;- ggplot(train_data,aes(capital.loss))+</div><div class="line">		geom_histogram(aes(y=..density..))+</div><div class="line">		geom_density(color=<span class="string">'green'</span>)</div><div class="line">grid.arrange(p1,p2)</div></pre></td></tr></table></figure></p>
<p><img src="http://onvolufm1.bkt.clouddn.com/histogram_capital.png" alt=""><br>可以发现，这两个特征的取值几乎全部集中在0这个值上，因此可以考虑从数据集中去除这两个特征。</p>
<h1 id="类失衡问题处理"><a href="#类失衡问题处理" class="headerlink" title="类失衡问题处理"></a>类失衡问题处理</h1><p>训练数据出现类失衡问题时，模型也会倾向于预测多数类，除了决策树和基于决策树的Bagging方法、Boosting方法受到类失衡数据的影响较小之外，一般需要对模型或者数据进行相应的处理。在能够进一步获取训练数据的前提下，采集更多的数据看类失衡的情况是否能够得到缓解。另外就是需要更换模型的评判方式，一般的准确率和误差率不再适用，而应该选择混淆矩阵、精确率、召回率和F1值等评价指标。</p>
<h2 id="样本权重"><a href="#样本权重" class="headerlink" title="样本权重"></a>样本权重</h2><p>增加少数类的样本权重，减少多数类的样本权重，类似于重复使用少数类样本。这种方法不对数据集进行抽样，而是在模型中指定样本的权重向量。</p>
<h2 id="抽样方法"><a href="#抽样方法" class="headerlink" title="抽样方法"></a>抽样方法</h2><p>抽样方法的思路不是让模型取应对类失衡问题，而是通过抽样方法手动平衡各个类别的样本比例。一般有向下抽样和向上抽样两种方法。</p>
<h3 id="向下抽样"><a href="#向下抽样" class="headerlink" title="向下抽样"></a>向下抽样</h3><p>从多数类别样本中抽取样本使得不同类别样本量大致相同。一种方法是从多数类样本中随机抽取直到样本量大致相同;另一种方法是对所有类使用bootstrap抽样使得bootstrap样本集中的类分布大致平衡。</p>
<h3 id="向上抽样"><a href="#向上抽样" class="headerlink" title="向上抽样"></a>向上抽样</h3><p>对少数类的样本进行<strong>有放回</strong>抽样直至不同类的样本量大致相同，类似于对样本赋予不一样的权重。</p>
<h2 id="截点选择"><a href="#截点选择" class="headerlink" title="截点选择"></a>截点选择</h2><p>通过ROC曲线选择模型分类的最佳截点值(即预测门槛值)，避免模型在少数类上的过高误分类率。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol>
<li>Max Kuhn. Applied Predictive Modeling</li>
<li>Trevor Hastie. The Elements of Statistical Learning, Data Mining, Inference, and Prediction, second Edition</li>
<li><a href="http://cs.ananas.chaoxing.com/download/554f2029e4b017d277eb4bf7" target="_blank" rel="external">http://cs.ananas.chaoxing.com/download/554f2029e4b017d277eb4bf7</a></li>
<li><a href="https://ccjou.wordpress.com/2013/04/15/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/" target="_blank" rel="external">https://ccjou.wordpress.com/2013/04/15/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/</a></li>
<li><a href="https://ccjou.wordpress.com/2014/03/14/%E8%B2%BB%E9%9B%AA%E7%9A%84%E5%88%A4%E5%88%A5%E5%88%86%E6%9E%90%E8%88%87%E7%B7%9A%E6%80%A7%E5%88%A4%E5%88%A5%E5%88%86%E6%9E%90/" target="_blank" rel="external">https://ccjou.wordpress.com/2014/03/14/%E8%B2%BB%E9%9B%AA%E7%9A%84%E5%88%A4%E5%88%A5%E5%88%86%E6%9E%90%E8%88%87%E7%B7%9A%E6%80%A7%E5%88%A4%E5%88%A5%E5%88%86%E6%9E%90/</a></li>
<li><a href="http://chenrudan.github.io/blog/2016/04/01/dimensionalityreduction.html#3.3" target="_blank" rel="external">http://chenrudan.github.io/blog/2016/04/01/dimensionalityreduction.html#3.3</a></li>
<li><a href="http://ufldl.stanford.edu/wiki/index.php/%E8%87%AA%E7%BC%96%E7%A0%81%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%80%E7%96%8F%E6%80%A7" target="_blank" rel="external">http://ufldl.stanford.edu/wiki/index.php/%E8%87%AA%E7%BC%96%E7%A0%81%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%80%E7%96%8F%E6%80%A7</a></li>
<li><a href="https://ccjou.wordpress.com/2013/05/29/%E5%8F%A4%E5%85%B8%E5%A4%9A%E7%B6%AD%E6%A8%99%E5%BA%A6%E6%B3%95-mds/" target="_blank" rel="external">https://ccjou.wordpress.com/2013/05/29/%E5%8F%A4%E5%85%B8%E5%A4%9A%E7%B6%AD%E6%A8%99%E5%BA%A6%E6%B3%95-mds/</a></li>
<li><a href="http://qf6101.github.io/machine%20learning/2015/08/01/Isolation-Forest" target="_blank" rel="external">http://qf6101.github.io/machine%20learning/2015/08/01/Isolation-Forest</a></li>
<li>SMS Spam Dataset:<a href="https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection" target="_blank" rel="external">https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection</a></li>
<li>LOF Example：<a href="http://www.cse.ust.hk/~leichen/courses/comp5331/lectures/LOF_Example.pdf" target="_blank" rel="external">http://www.cse.ust.hk/~leichen/courses/comp5331/lectures/LOF_Example.pdf</a></li>
</ol>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/标签/数据清洗/" rel="tag"># 数据清洗</a>
          
            <a href="/标签/数据预处理/" rel="tag"># 数据预处理</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/04/03/SVM模型的由来及SMO算法的python实现/" rel="next" title="SVM模型的由来及SMO算法的python实现">
                <i class="fa fa-chevron-left"></i> SVM模型的由来及SMO算法的python实现
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/04/08/一个英文文本分类例子/" rel="prev" title="一个英文文本分类的例子">
                一个英文文本分类的例子 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="http://onvolufm1.bkt.clouddn.com/avatar.jpg"
               alt="周君君" />
          <p class="site-author-name" itemprop="name">周君君</p>
           
              <p class="site-description motion-element" itemprop="description">越努力越幸运</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">11</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">分类</span>
              
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">16</span>
                <span class="site-state-item-name">标签</span>
              
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/zhoujunjun-apple" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#量纲标准化"><span class="nav-number">1.</span> <span class="nav-text">量纲标准化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#问题描述"><span class="nav-number">1.1.</span> <span class="nav-text">问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#处理方法"><span class="nav-number">1.2.</span> <span class="nav-text">处理方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#优缺点"><span class="nav-number">1.3.</span> <span class="nav-text">优缺点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#例子"><span class="nav-number">1.4.</span> <span class="nav-text">例子</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#有偏数据的检测和处理"><span class="nav-number">2.</span> <span class="nav-text">有偏数据的检测和处理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#问题描述-1"><span class="nav-number">2.1.</span> <span class="nav-text">问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#检测与处理方法"><span class="nav-number">2.2.</span> <span class="nav-text">检测与处理方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#例子-1"><span class="nav-number">2.3.</span> <span class="nav-text">例子</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#离群点的检测和处理"><span class="nav-number">3.</span> <span class="nav-text">离群点的检测和处理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#问题描述-2"><span class="nav-number">3.1.</span> <span class="nav-text">问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#检测方法"><span class="nav-number">3.2.</span> <span class="nav-text">检测方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Robust-Convariance"><span class="nav-number">3.2.1.</span> <span class="nav-text">Robust Convariance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#One-Class-SVM"><span class="nav-number">3.2.2.</span> <span class="nav-text">One_Class SVM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Isolation-Forest"><span class="nav-number">3.2.3.</span> <span class="nav-text">Isolation Forest</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#iForest例子"><span class="nav-number">3.2.3.1.</span> <span class="nav-text">iForest例子</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Local-Outlier-Factor"><span class="nav-number">3.2.4.</span> <span class="nav-text">Local Outlier Factor</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#处理方法-1"><span class="nav-number">3.3.</span> <span class="nav-text">处理方法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#降维和特征选择"><span class="nav-number">4.</span> <span class="nav-text">降维和特征选择</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#问题描述-3"><span class="nav-number">4.1.</span> <span class="nav-text">问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#方法"><span class="nav-number">4.2.</span> <span class="nav-text">方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#PCA"><span class="nav-number">4.2.1.</span> <span class="nav-text">PCA</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#原理"><span class="nav-number">4.2.1.1.</span> <span class="nav-text">原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#优劣势"><span class="nav-number">4.2.1.2.</span> <span class="nav-text">优劣势</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PLS"><span class="nav-number">4.2.2.</span> <span class="nav-text">PLS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Fisher’s-Discriminant-Analysis"><span class="nav-number">4.2.3.</span> <span class="nav-text">Fisher’s Discriminant Analysis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#自编码器"><span class="nav-number">4.2.4.</span> <span class="nav-text">自编码器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#其他"><span class="nav-number">4.2.5.</span> <span class="nav-text">其他</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#例子-2"><span class="nav-number">4.3.</span> <span class="nav-text">例子</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#缺失值处理"><span class="nav-number">5.</span> <span class="nav-text">缺失值处理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#例子-3"><span class="nav-number">5.1.</span> <span class="nav-text">例子</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#预测特征的移除"><span class="nav-number">6.</span> <span class="nav-text">预测特征的移除</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#问题说明"><span class="nav-number">6.1.</span> <span class="nav-text">问题说明</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#共线性检测"><span class="nav-number">6.2.</span> <span class="nav-text">共线性检测</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#退化分布的预测特征的检测"><span class="nav-number">6.3.</span> <span class="nav-text">退化分布的预测特征的检测</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#例子-4"><span class="nav-number">6.4.</span> <span class="nav-text">例子</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#类失衡问题处理"><span class="nav-number">7.</span> <span class="nav-text">类失衡问题处理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#样本权重"><span class="nav-number">7.1.</span> <span class="nav-text">样本权重</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#抽样方法"><span class="nav-number">7.2.</span> <span class="nav-text">抽样方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#向下抽样"><span class="nav-number">7.2.1.</span> <span class="nav-text">向下抽样</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#向上抽样"><span class="nav-number">7.2.2.</span> <span class="nav-text">向上抽样</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#截点选择"><span class="nav-number">7.3.</span> <span class="nav-text">截点选择</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考文献"><span class="nav-number">8.</span> <span class="nav-text">参考文献</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">周君君</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  






  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  


  




	





  





  





  






  





  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

</body>
</html>
